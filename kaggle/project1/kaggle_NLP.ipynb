{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approaching (almost) any NLP problems \n",
    "\n",
    "https://www.kaggle.com/code/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # jax or torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import xgboost as xgb \n",
    "from tqdm import  tqdm_gui, tqdm_pandas, tqdm_notebook, tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "\n",
    "from keras.models import  Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import  Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import  to_categorical\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import  sequence\n",
    "from keras.callbacks import  EarlyStopping\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk import  word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n"
     ]
    }
   ],
   "source": [
    "# loading datasets\n",
    "\n",
    "train = pd.read_csv(\"./input/spooky/train.csv\")\n",
    "test = pd.read_csv(\"./input/spooky/test.csv\")\n",
    "sample = pd.read_csv(\"./input/spooky/sample_submission.csv\")\n",
    "print(train.shape, test.shape, sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'HPL', 'MWS'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "    \n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labelEncoder\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train.text.values,\n",
    "    y,\n",
    "    stratify=y,\n",
    "    random_state = 42,\n",
    "    test_size=0.1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1958,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Her hair was the brightest living gold, and despite the poverty of her clothing, seemed to set a crown of distinction on her head.',\n",
       "       '\"No,\" he said, \"oh, no a member of my family my niece, and a most accomplished woman.\"',\n",
       "       'The magistrate appeared at first perfectly incredulous, but as I continued he became more attentive and interested; I saw him sometimes shudder with horror; at others a lively surprise, unmingled with disbelief, was painted on his countenance.',\n",
       "       ...,\n",
       "       'The medical testimony spoke confidently of the virtuous character of the deceased.',\n",
       "       'When we arrived, after a little rest, he led me over the house and pointed out to me the rooms which my mother had inhabited.',\n",
       "       'Some were destroyed; the major part escaped by quick and well ordered movements; and danger made them careful.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thomas\" turning to me \"is decidedly the best hand at a cork leg; but if you should ever want an arm, my dear fellow, you must really let me recommend you to Bishop.\"',\n",
       "       'I struggled to reason off the nervousness which had dominion over me.',\n",
       "       'My name, indeed, has been so long and so constantly before the public eye, that I am not only willing to admit the naturalness of the interest which it has everywhere excited, but ready to satisfy the extreme curiosity which it has inspired.',\n",
       "       ...,\n",
       "       'Such is human nature, that beauty and deformity are often closely linked.',\n",
       "       'He had sought this office with eagerness, under the idea of turning his whole forces to the suppression of the privileged orders of our community.',\n",
       "       'Especially was it unwise to rave of the living things that might haunt such a place; of creatures half of the jungle and half of the impiously aged city fabulous creatures which even a Pliny might describe with scepticism; things that might have sprung up after the great apes had overrun the dying city with the walls and the pillars, the vaults and the weird carvings.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text embedding\n",
    "\n",
    "tfv = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_features=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"\\w{1,}\",\n",
    "    ngram_range=(1, 3),\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17621, 15102), (1958, 15102))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv = tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtrain_tfv.shape, xvalid_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x15102 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 198521 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.570\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count as word embedding\n",
    "\n",
    "ctv = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"\\w{1,}\",\n",
    "    ngram_range=(1,3),\n",
    "    stop_words=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17621, 400266), (1958, 400266))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "xtrain_ctv.shape, xvalid_ctv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes model\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes model\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 15102) (1958, 15102)\n",
      "(17621, 120) (1958, 120)\n"
     ]
    }
   ],
   "source": [
    "# SVM needs dimensionality reduction and standardization of the data\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "print(xtrain_tfv.shape, xvalid_tfv.shape)\n",
    "print(xtrain_svd.shape, xvalid_svd.shape)\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.733\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, probability=True)\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.783\n"
     ]
    }
   ],
   "source": [
    "# use xgboost --- embedding #1\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    nthread=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.774\n"
     ]
    }
   ],
   "source": [
    "# use xgboost --- embedding #2\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    nthread=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.773\n"
     ]
    }
   ],
   "source": [
    "# use xgboost --- embedding with dimensionality reduction\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    nthread=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.773\n"
     ]
    }
   ],
   "source": [
    "# use xgboost --- embedding with dimensionality reduction and standadization\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    nthread=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(\n",
    "    multiclass_logloss,\n",
    "    greater_is_better=False,\n",
    "    response_method=\"predict_proba\" # needs_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline construction\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "    (\"svd\", svd),\n",
    "    (\"scl\", scl),\n",
    "    (\"lr\", lr_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svd__n_components\": [120, 180],\n",
    "    \"lr__C\": [0.1, 1.0, 10], \n",
    "    \"lr__penalty\": [\"l1\", \"l2\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV 1/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.0s\n",
      "[CV 1/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 1/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.1s\n",
      "[CV 2/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 2/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120;, score=-0.778 total time=   1.1s\n",
      "[CV 1/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120;, score=-0.776 total time=   1.2s\n",
      "[CV 2/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.2s\n",
      "[CV 2/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.3s\n",
      "[CV 1/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120;, score=-0.779 total time=   1.2s\n",
      "[CV 2/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120;, score=-0.773 total time=   1.4s\n",
      "[CV 2/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180;, score=nan total time=   2.2s\n",
      "[CV 1/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.2s\n",
      "[CV 2/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120;, score=nan total time=   1.2s\n",
      "[CV 1/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180;, score=nan total time=   2.3s\n",
      "[CV 2/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180;, score=nan total time=   2.3s\n",
      "[CV 2/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180;, score=nan total time=   2.3s\n",
      "[CV 2/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180;, score=-0.731 total time=   2.4s\n",
      "[CV 1/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180;, score=-0.738 total time=   2.5s\n",
      "[CV 1/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120;, score=-0.770 total time=   1.2s\n",
      "[CV 2/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120;, score=-0.774 total time=   1.2s\n",
      "[CV 1/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180;, score=-0.742 total time=   2.4s\n",
      "[CV 2/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180;, score=-0.744 total time=   2.4s\n",
      "[CV 1/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180;, score=nan total time=   1.5s\n",
      "[CV 2/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180;, score=nan total time=   1.4s\n",
      "[CV 1/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180;, score=-0.740 total time=   1.4s\n",
      "[CV 2/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180;, score=-0.739 total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.77694726 -0.73422326         nan         nan\n",
      " -0.77599641 -0.74316277         nan         nan -0.77197449 -0.73973158]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: -0.734\n",
      "best parameter set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=mll_scorer,\n",
    "    verbose=10,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    cv=2\n",
    ")\n",
    "\n",
    "model.fit(xtrain_tfv, ytrain)\n",
    "print(\"best score: %.3f\"%model.best_score_)\n",
    "print(\"best parameter set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\"%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV 2/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 1/2; 1/6] START nb__alpha=0.001.............................................\n",
      "[CV 2/2; 1/6] END .............nb__alpha=0.001;, score=-0.641 total time=   0.0s\n",
      "[CV 1/2; 1/6] END .............nb__alpha=0.001;, score=-0.620 total time=   0.0s\n",
      "[CV 1/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 1/2; 2/6] END ..............nb__alpha=0.01;, score=-0.511 total time=   0.0s\n",
      "[CV 2/2; 2/6] START nb__alpha=0.01..............................................\n",
      "[CV 2/2; 2/6] END ..............nb__alpha=0.01;, score=-0.523 total time=   0.0s\n",
      "[CV 2/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] START nb__alpha=0.1...............................................\n",
      "[CV 1/2; 3/6] END ...............nb__alpha=0.1;, score=-0.489 total time=   0.0s\n",
      "[CV 2/2; 3/6] END ...............nb__alpha=0.1;, score=-0.495 total time=   0.0s\n",
      "[CV 1/2; 5/6] START nb__alpha=10................................................\n",
      "[CV 2/2; 4/6] START nb__alpha=1.0...............................................[CV 2/2; 5/6] START nb__alpha=10................................................\n",
      "\n",
      "[CV 1/2; 4/6] START nb__alpha=1.0...............................................\n",
      "[CV 1/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 2/2; 6/6] START nb__alpha=100...............................................\n",
      "[CV 1/2; 5/6] END ................nb__alpha=10;, score=-0.950 total time=   0.0s\n",
      "[CV 2/2; 5/6] END ................nb__alpha=10;, score=-0.951 total time=   0.0s\n",
      "[CV 1/2; 4/6] END ...............nb__alpha=1.0;, score=-0.663 total time=   0.0s\n",
      "[CV 2/2; 4/6] END ...............nb__alpha=1.0;, score=-0.666 total time=   0.0s\n",
      "[CV 1/2; 6/6] END ...............nb__alpha=100;, score=-1.067 total time=   0.0s\n",
      "[CV 2/2; 6/6] END ...............nb__alpha=100;, score=-1.067 total time=   0.0s\n",
      "best score: -0.492\n",
      "best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "    (\"nb\", nb_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"nb__alpha\": [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=mll_scorer,\n",
    "    verbose=10,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    cv=2\n",
    ")\n",
    "\n",
    "model.fit(xtrain_tfv, ytrain)\n",
    "print(\"best score: %.3f\"%model.best_score_)\n",
    "print(\"best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\"%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:33, 65456.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND 2195884 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(\"./input/glove.840B.300d.txt\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    values = [i for i in values[1:] if type(i) == \"float32\"] # i.isnumeric()] # type(i) == \"float32\"\n",
    "    coefs = np.asarray(values, dtype=\"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"FOUND %s word vectors\"%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower().encode(\"utf-8\").decode(\"utf-8\")\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try: \n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != \"np.ndarray\":\n",
    "        return np.zeros(300)\n",
    "    return v / np.linalg.norm(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17621/17621 [00:01<00:00, 10839.18it/s]\n",
      "100%|██████████| 1958/1958 [00:00<00:00, 10639.73it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17621, 300), (1958, 300))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)\n",
    "\n",
    "xtrain_glove.shape, xvalid_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.088\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.088\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    nthread=10,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print(\"logloss: %.3f\" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "xtrain_glove_scl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize the lables for the neural network\n",
    "\n",
    "ytrain_enc = to_categorical(ytrain)\n",
    "yvalid_enc = to_categorical(yvalid)\n",
    "yvalid_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# create a simple 3 layer sequential neural network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0885 - val_loss: 1.0885\n",
      "Epoch 2/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0876 - val_loss: 1.0878\n",
      "Epoch 3/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0870 - val_loss: 1.0878\n",
      "Epoch 4/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0895 - val_loss: 1.0878\n",
      "Epoch 5/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0889 - val_loss: 1.0880\n",
      "Epoch 6/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0891 - val_loss: 1.0877\n",
      "Epoch 7/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0878 - val_loss: 1.0878\n",
      "Epoch 8/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0874 - val_loss: 1.0880\n",
      "Epoch 9/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0866 - val_loss: 1.0877\n",
      "Epoch 10/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0884 - val_loss: 1.0876\n",
      "Epoch 11/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0885 - val_loss: 1.0877\n",
      "Epoch 12/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0871 - val_loss: 1.0878\n",
      "Epoch 13/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0881 - val_loss: 1.0877\n",
      "Epoch 14/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0878 - val_loss: 1.0876\n",
      "Epoch 15/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0887 - val_loss: 1.0877\n",
      "Epoch 16/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0858 - val_loss: 1.0878\n",
      "Epoch 17/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0864 - val_loss: 1.0879\n",
      "Epoch 18/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0874 - val_loss: 1.0878\n",
      "Epoch 19/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0887 - val_loss: 1.0877\n",
      "Epoch 20/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0899 - val_loss: 1.0877\n",
      "Epoch 21/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0887 - val_loss: 1.0877\n",
      "Epoch 22/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0863 - val_loss: 1.0877\n",
      "Epoch 23/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0886 - val_loss: 1.0877\n",
      "Epoch 24/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0901 - val_loss: 1.0879\n",
      "Epoch 25/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0885 - val_loss: 1.0877\n",
      "Epoch 26/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0888 - val_loss: 1.0876\n",
      "Epoch 27/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0903 - val_loss: 1.0876\n",
      "Epoch 28/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0873 - val_loss: 1.0878\n",
      "Epoch 29/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0862 - val_loss: 1.0877\n",
      "Epoch 30/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0873 - val_loss: 1.0877\n",
      "Epoch 31/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0887 - val_loss: 1.0877\n",
      "Epoch 32/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0853 - val_loss: 1.0877\n",
      "Epoch 33/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0891 - val_loss: 1.0885\n",
      "Epoch 34/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0878 - val_loss: 1.0877\n",
      "Epoch 35/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0883 - val_loss: 1.0877\n",
      "Epoch 36/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0883 - val_loss: 1.0877\n",
      "Epoch 37/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0879 - val_loss: 1.0877\n",
      "Epoch 38/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0889 - val_loss: 1.0877\n",
      "Epoch 39/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0882 - val_loss: 1.0877\n",
      "Epoch 40/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0883 - val_loss: 1.0877\n",
      "Epoch 41/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0870 - val_loss: 1.0876\n",
      "Epoch 42/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0865 - val_loss: 1.0876\n",
      "Epoch 43/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0870 - val_loss: 1.0877\n",
      "Epoch 44/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0882 - val_loss: 1.0876\n",
      "Epoch 45/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0869 - val_loss: 1.0877\n",
      "Epoch 46/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0890 - val_loss: 1.0878\n",
      "Epoch 47/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0885 - val_loss: 1.0876\n",
      "Epoch 48/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0870 - val_loss: 1.0877\n",
      "Epoch 49/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0891 - val_loss: 1.0877\n",
      "Epoch 50/50\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0880 - val_loss: 1.0877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x41683c310>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    xtrain_glove_scl,\n",
    "    y=ytrain_enc,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_data=(xvalid_glove_scl, yvalid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer to get the embedding matrix [word --> vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer for LSTM\n",
    "\n",
    "token = Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the squences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25943, dict)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index), type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621, 70)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29,\n",
       " 560,\n",
       " 8,\n",
       " 1,\n",
       " 5924,\n",
       " 459,\n",
       " 714,\n",
       " 3,\n",
       " 987,\n",
       " 1,\n",
       " 1794,\n",
       " 2,\n",
       " 29,\n",
       " 3695,\n",
       " 98,\n",
       " 4,\n",
       " 326,\n",
       " 5,\n",
       " 2545,\n",
       " 2,\n",
       " 3103,\n",
       " 27,\n",
       " 29,\n",
       " 166]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,   29,  560,    8,    1, 5924,  459,  714,    3,  987,\n",
       "          1, 1794,    2,   29, 3695,   98,    4,  326,    5, 2545,    2,\n",
       "       3103,   27,   29,  166], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25943 [00:00<?, ?it/s]/var/folders/bm/wkp8f2_14dbc7fslvm1mlyvc0000gn/T/ipykernel_6470/2008892693.py:6: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if embedding_vector:\n",
      "100%|██████████| 25943/25943 [00:00<00:00, 1240230.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix for the words we have in the dataset\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector:\n",
    "        embedding_matrix[i, :] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25944, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# a simple LSTM with glove embeddings and two dense layers\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "layer =  Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        trainable=False,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_shape=(max_len,)\n",
    "    )\n",
    "#layer.set_weights(embedding_matrix)\n",
    "\n",
    "model.add(layer)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 284ms/step - loss: 1.0871 - val_loss: 1.0878\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 290ms/step - loss: 1.0864 - val_loss: 1.0877\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 284ms/step - loss: 1.0868 - val_loss: 1.0877\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 248ms/step - loss: 1.0876 - val_loss: 1.0876\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 222ms/step - loss: 1.0872 - val_loss: 1.0876\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 249ms/step - loss: 1.0873 - val_loss: 1.0876\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 287ms/step - loss: 1.0865 - val_loss: 1.0876\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 233ms/step - loss: 1.0865 - val_loss: 1.0876\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 282ms/step - loss: 1.0884 - val_loss: 1.0876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x419266050>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    xtrain_pad,\n",
    "    y=ytrain_enc,\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc),\n",
    "    callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 517ms/step - loss: 1.0975 - val_loss: 1.0948\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 524ms/step - loss: 1.0937 - val_loss: 1.0921\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 520ms/step - loss: 1.0917 - val_loss: 1.0903\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 527ms/step - loss: 1.0903 - val_loss: 1.0892\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 525ms/step - loss: 1.0877 - val_loss: 1.0884\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 519ms/step - loss: 1.0881 - val_loss: 1.0881\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 526ms/step - loss: 1.0871 - val_loss: 1.0878\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 520ms/step - loss: 1.0874 - val_loss: 1.0877\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 524ms/step - loss: 1.0886 - val_loss: 1.0877\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 518ms/step - loss: 1.0890 - val_loss: 1.0877\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 524ms/step - loss: 1.0876 - val_loss: 1.0876\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 520ms/step - loss: 1.0870 - val_loss: 1.0876\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 527ms/step - loss: 1.0883 - val_loss: 1.0876\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 527ms/step - loss: 1.0870 - val_loss: 1.0876\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 527ms/step - loss: 1.0865 - val_loss: 1.0876\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 531ms/step - loss: 1.0884 - val_loss: 1.0876\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 525ms/step - loss: 1.0873 - val_loss: 1.0876\n",
      "Epoch 18/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 523ms/step - loss: 1.0879 - val_loss: 1.0876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x415464cd0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple LSTM with glove embeddings and two dense layers\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "layer =  Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        trainable=False,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_shape=(max_len,)\n",
    "    )\n",
    "#layer.set_weights(embedding_matrix)\n",
    "\n",
    "model.add(layer)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    xtrain_pad,\n",
    "    y=ytrain_enc,\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc),\n",
    "    callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ychu/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 687ms/step - loss: 1.0975 - val_loss: 1.0948\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 680ms/step - loss: 1.0941 - val_loss: 1.0921\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 674ms/step - loss: 1.0916 - val_loss: 1.0902\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 675ms/step - loss: 1.0894 - val_loss: 1.0890\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 673ms/step - loss: 1.0881 - val_loss: 1.0883\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 676ms/step - loss: 1.0898 - val_loss: 1.0881\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 695ms/step - loss: 1.0875 - val_loss: 1.0878\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 687ms/step - loss: 1.0868 - val_loss: 1.0877\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 683ms/step - loss: 1.0866 - val_loss: 1.0877\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 689ms/step - loss: 1.0898 - val_loss: 1.0877\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 686ms/step - loss: 1.0872 - val_loss: 1.0876\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 686ms/step - loss: 1.0891 - val_loss: 1.0876\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 689ms/step - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 699ms/step - loss: 1.0850 - val_loss: 1.0876\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 698ms/step - loss: 1.0877 - val_loss: 1.0876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x37009ecd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple LSTM with glove embeddings and two dense layers\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "layer =  Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        trainable=False,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_shape=(max_len,)\n",
    "    )\n",
    "#layer.set_weights(embedding_matrix)\n",
    "\n",
    "model.add(layer)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"auto\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    xtrain_pad,\n",
    "    y=ytrain_enc,\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc),\n",
    "    callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import os, sys, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stdout\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type=\"classification\", optimize=roc_auc_score,\n",
    "                lower_is_better=False, save_path=None):\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "    \n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        self.training_data = training_data\n",
    "        self.y = y \n",
    "\n",
    "        if self.task_type == \"classification\":\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(f\"Found {self.num_classes} classes\")\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "        \n",
    "        self.train_prediciton_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros(\n",
    "                (train_prediction_shape[0],\n",
    "                 train_prediction_shape[1] * len(self.model_dict[level]))\n",
    "            )\n",
    "    \n",
    "        for level in range(self.levels):\n",
    "            if level==0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "            \n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(f\"Training level {level} Fold # {foldnum}. model {model_num}\")\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    logger.info(f\"predicting level {level}. Fold {foldnum}. Model {model_num}\")\n",
    "\n",
    "                    if self.task_type == \"classification\":\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num*self.num_classes) + self.num_classes] = temp_train_predictions\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    \n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(f\"level {level}, Fold {foldnum}. Model {model_num}. Validation score={validation_score}\")\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(f\"level {level}. Model {model_num}. Mean score={avg_score}. Std Dev={std_score}\")\n",
    "            logger.info(f\"saving predictions for level {level}\")\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\"+str(level)+\".csv\"),\n",
    "                                        index=False, header=None)\n",
    "        return self.train_prediction_dict\n",
    "    \n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == \"classification\":\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "        \n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((\n",
    "                test_prediction_shape[0],\n",
    "                test_prediction_shape[1] * len(self.model_dict[level])\n",
    "            ))\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        for level in range(self.levels):\n",
    "            if level==0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "            \n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                logger.info(f\"training fulldata level {level}. model {model}\")\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "                \n",
    "                logger.info(f\"predicting test level {level}. Model {model_num}\")\n",
    "                if self.task_type == \"classification\":\n",
    "                    if level==0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    \n",
    "                    self.test_prediction_dict[level][:, (model_num*self.num_classes): (model_num*self.num_classes)+self.num_classes] = temp_test_predictions\n",
    "                else:\n",
    "                    if level ==0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(\n",
    "                os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                index=False, header=None\n",
    "            )\n",
    "        return self.test_predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1:[xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1:[xvalid_glove]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "    1: [xgb.XGBClassifier(n_estimator=120, max_depth=7)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = Ensembler(\n",
    "    model_dict = model_dict,\n",
    "    num_folds = 3,\n",
    "    task_type = \"classification\",\n",
    "    optimize = multiclass_logloss,\n",
    "    lower_is_better = True,\n",
    "    save_path = ''\n",
    ")\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_logloss(yvalid, preds[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
